#coding=utf-8
#Version:python3.6.0
_date_ = '2019/7/10 12:36'
_author_ = 'tan'
#模块化设计
#神经网络搭建的八股
import tensorflow as tf

#正则化权重regularizer
def forward(x, regularizer):
    w =
    b =
    y =
    return y

#w的shape
def get_weight(shape, regularizer):
    w = tf.Variable()
    #把每个正则化w的损失加到总损失losses
    tf.add_to_collection('losses', tf.contrib.l2_regularizer(regularizer)(w))
    return w

def get_bias(shape):
    b = tf.Variable()
    return b

def backward():
    x = tf.placeholder()
    y = tf.placeholder()
    y=  forward.forward(x, REGULARIZER)
    global_step = tf.Variable(0, trainable=False)

    loss =
    loss可以是：
    y与y_的距离(loss_mse) = tf.reduce_mean(tf.square(y - y_))
    也可以是：
    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y ,labels=tf.argmax(y_, 1))
    y与y_的距离(loss_mse) = tf.reduce_mean(ce)

    #加入正则化后
    loss = y与y_的距离有+tf.add_n(tf.get_collection('losses'))

    #指数衰减学习率
    learning_rate = tf.train.exponential_decay(
        LEARNNING_RATE_BASE,
        global_step,
        数据集总样本/BATCH_SIZE,
        LEARNNING_RATE_DECAY,
        staircase=True
    )
    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)

    #滑动平均
    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)
    ema_op =  ema.apply(tf.trainable_variables())
    with tf.control_dependencies([train_step,ema_op]):
        train_op = tf.no_op(name='train')

    #初始化所有参数
    with tf.Session() as sess:
        init_op = tf.global_variables_initializer()
        sess.run(init_op)

        for i in range(STEPS):
            sess.run(train_step, feed_dict=={x: ,y_:})
            if i % 轮数 == 0:
                print()

if __name__ == '__main__':
    backward()